{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6084c2ec-dc01-4fee-80c4-dec8e0214ce1",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "RFE, PCA, PPCA and all of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665d23a-1b1d-44ab-ae98-226e29f9c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import importlib\n",
    "from ppca import PPCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "import utils\n",
    "from utils.testers import ss, assess\n",
    "from utils.reporters import basic_scores, bs_rankings, bs_curves\n",
    "from utils.helpers import mask50, plabels\n",
    "importlib.reload(utils.reporters)\n",
    "importlib.reload(utils.testers)\n",
    "importlib.reload(utils.helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3568d-8038-448a-8ade-44ddc66d4d06",
   "metadata": {},
   "source": [
    "### Feature reduction\n",
    "- RFE(CV) with 50 features is the single best thing to help the periodic classifier\n",
    "- For assessment see the PCA section\n",
    "- Apply just RFE to the other sets to see the list of features. A bit of science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e083e57-9b15-4303-b9ad-c9755cb3886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset import\n",
    "raw_data_path = \"data/pkl/divided/\"\n",
    "\n",
    "tf = pd.read_pickle(raw_data_path+\"transient_features.pkl\")\n",
    "tl = pd.read_pickle(raw_data_path+\"transient_labeled.pkl\")\n",
    "sf = pd.read_pickle(raw_data_path+\"stochastic_features.pkl\")\n",
    "sl = pd.read_pickle(raw_data_path+\"stochastic_labeled.pkl\")\n",
    "pf = pd.read_pickle(raw_data_path+\"periodic_features.pkl\")\n",
    "pl = pd.read_pickle(raw_data_path+\"periodic_labeled.pkl\")\n",
    "\n",
    "tme = myLabelEncoder(tlabels)\n",
    "sme = myLabelEncoder(slabels)\n",
    "pme = myLabelEncoder(plabels)\n",
    "\n",
    "si = SimpleImputer(strategy=\"constant\",fill_value=-999)\n",
    "\n",
    "Xt = si.fit_transform(tf.values)\n",
    "Xs = si.fit_transform(sf.values)\n",
    "Xp = si.fit_transform(pf.values)\n",
    "\n",
    "yt = tme.fit_transform(tl[\"classALeRCE\"].values)\n",
    "ys = sme.fit_transform(sl[\"classALeRCE\"].values)\n",
    "yp = pme.fit_transform(pl[\"classALeRCE\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090292b2-f6cb-4d22-9eca-b70b3c106d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute and save RFECV transforms\n",
    "compute_save_rfecv(Xt, yt, \"data/pkl/rfe/rfe-transient.pkl\")\n",
    "compute_save_rfecv(Xs, ys, \"data/pkl/rfe/rfs-stochastic.pkl\")\n",
    "compute_save_rfecv(Xp, yp, \"data/pkl/rfe/rfe-periodic.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3880d9f-7fe6-4584-aa75-51a3d45accd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8229b2f-0224-4270-abf5-b35a0e2fd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the features selected. \n",
    "# Present in one, two or three classes\n",
    "# Comment scientifically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04d110-0c66-4c2d-9744-548331f9263e",
   "metadata": {},
   "source": [
    "### PCA\n",
    "## Intro\n",
    "- Data is full of NaNs: we have to interpolate using a constant -999 (paper) or other ideas (mean, median...)\n",
    "- Probabilistic PCA iteratively tries to get a good set of principal components starting from a random one,\n",
    "  interpolating the data meanwhile\n",
    "- Just a better imputer or the rotation is useful? Data suggests it does not really help\n",
    "- References: pca-magic, paper (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe9c64-f201-449c-9726-f19131072482",
   "metadata": {},
   "source": [
    "## PPCA transformation vs simple interpolating (ppcas1/):\n",
    "- PPCA-rotating we get pretty distanced 0.92 and 0.94 roc_auc for full and rfe-reduced dataset largely independent of number components assumed during PPCA\n",
    "- Just using the interpolated data but without caring about principal components the score goes up to 0.972 and 0.974 respectively, so better.\n",
    "- Try different random_states, just in case, but I think this proves PPCA is just good enough to fill\n",
    "- Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d939f-7653-4918-bea7-65d62a2128d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_save_ppca(X, d=[10, 40, 50, 100, 150, 183)\n",
    "# compute_save_ppca(X50, d=[10, 40, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1401e82-2846-48d6-a00b-b1dbdd7b3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datasets\n",
    "# scores, scores_stds = basic_scores()\n",
    "# bs_rankings(scores, scores_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a912fae-da19-40c5-9971-00f676c55933",
   "metadata": {},
   "source": [
    "## Minimum to maximum number of assumed principal components (2->50) (ppcas2/):\n",
    "- Focusing on the rfe-reduced dataset, it's interesting that even 10 components (out of 50 features) give a really good result\n",
    "- Either way the best result is obtained assuming as many underlying components as features (50)\n",
    "- Curve plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcd805-080b-48ea-b3b8-fc91b5be5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_save_ppca(X50, d=[2,...,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb12a7-5fa1-4419-b9fd-79864090f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datasets\n",
    "# scores, scores_stds = basic_scores()\n",
    "# bs_curves(scores, scores_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af81b44-7379-437a-b887-2c79463f0032",
   "metadata": {},
   "source": [
    "## Raw dataset vs rfe vs ppca imputing (ppcas3/):\n",
    "- Proves that according to every metric but my frobenius_score the rfe-reduced set wins, but there isn't a clear winner between -999 filling and PPCA interpolation without rotation, so simplicity suggests just using constant value imputing\n",
    "- Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7adf8e-45b8-4336-bd83-5ee9b4440149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_save_ppca(x50, d=50)\n",
    "si = SimpleImputer(strategy=\"constant\", fillvalue=-999)\n",
    "X = si.fit_transform(pf.values)\n",
    "X50 = si.fit_transform(pf50.values)\n",
    "np.save(\"data/ppcas3/X/basicfull.npy\",X)\n",
    "np.save(\"data/ppcas3/X/basic50.npy\",X50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501c1f1-4072-4d33-ac8d-8eeee1f4d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 50\n",
    "n_estimators = 2000\n",
    "rotate = False\n",
    "\n",
    "test_datasets(\"ppcas3\")\n",
    "scores = basic_scores(\"ppcas3/*.pkl\")\n",
    "bs_rankings(scores)\n",
    "bs_rankings(scores,winners=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d9be3-fbad-4c04-9ac9-772ae5351f52",
   "metadata": {},
   "source": [
    "## Explained variances\n",
    "- Concretely: compare explained variances of PPCA, PCA on -999 and PCA on PPCA (I know, stupid, but try), is there anything useful?\n",
    "- More concretely: I want a plot of three curves of explained variance as a function of component rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd23e2-ca82-48e7-8aeb-3a186dd45b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve data from ppcas1\n",
    "# ppca_50_50.var_exp -> get ratios\n",
    "# perform PCA on -999 filled 50\n",
    "# perform PCA on PPCA filled 50/50\n",
    "# plot the three curves of explained variance ratio as a function of component rank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
